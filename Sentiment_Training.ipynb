{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import scipy.sparse as sp\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import create_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('glassdoor.csv', low_memory=False)\n",
    "\n",
    "# Remove rows with null values in our predictor and target variable columns\n",
    "df = df.dropna(subset=['PROs', 'CONs', 'Outlook Value'])\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = ['CUSIP', 'ISIN', 'SEDOL', 'As Of Date', 'Logo', 'Author Country', \n",
    "                     'Date Updated', 'Company Industry Id', 'Company Sector Id', 'Job Ending Year', 'Description', 'Not Helpful Count']\n",
    "df = df.drop(columns=columns_to_remove, errors='ignore')  # Avoid errors if a column doesn't exist\n",
    "\n",
    "# Convert 'Date Added' into a datetime format\n",
    "df['Date Added'] = pd.to_datetime(df['Date Added'].str[:8], format='%Y%m%d')\n",
    "\n",
    "# Remove duplicate reviews\n",
    "df = df.drop_duplicates(subset=['Review Url'])\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of distinct values for various fields\n",
    "distinct_values = df[['Company', 'Author Title', 'Author Location', 'Industry']].nunique()\n",
    "print(\"Number of distinct values in each field:\")\n",
    "print(distinct_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of ratings columns\n",
    "\n",
    "# Define the rating columns\n",
    "rating_columns = [\n",
    "    'Recommends Value', 'Outlook Value', 'CEO Review Value', \n",
    "    'Rating: Overall', 'Rating: Work/Life Balance', 'Rating: Culture & Values', \n",
    "    'Rating: Career Opportunities', 'Rating: Comp & Benefits', \n",
    "    'Rating: Senior Management', 'Rating: Diversity & Inclusion'\n",
    "]\n",
    "\n",
    "# Plot histograms for each rating column\n",
    "fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "axes = axes.ravel()  # Flatten the array for easier iteration\n",
    "\n",
    "for i, col in enumerate(rating_columns):\n",
    "    sns.histplot(df[col], bins=10, kde=False, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel('Rating')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix for numerical columns\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Generate a heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Remove non-printable characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove punctuations and numbers\n",
    "    text = re.sub(r'[\\W_]+', ' ', text, flags=re.UNICODE)\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to both 'PROs' and 'CONs' columns\n",
    "df['PROs_clean'] = df['PROs'].apply(clean_text)\n",
    "df['CONs_clean'] = df['CONs'].apply(clean_text)\n",
    "\n",
    "# Combine cleaned text columns\n",
    "df['combined_text'] = df['PROs_clean'] + \" \" + df['CONs_clean']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                                   stop_words=stopwords.words('english'),\n",
    "                                   max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the predictor as the combined text of PROs and CONs\n",
    "predictor = df['combined_text']\n",
    "\n",
    "# Vectorize text data\n",
    "X = tfidf_vectorizer.fit_transform(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "y = df['Outlook Value'].apply(lambda x: 1 if x == 1 else 0) # Make target robust to changes in case we add a neutral outlook option in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=10000, class_weight='balanced') # Use balanced class weights to handle potential class imbalance\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = lr_model.predict(X_train)\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Training Classification Report:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Check for overfitting\n",
    "if accuracy_score(y_train, y_train_pred) > accuracy_score(y_test, y_test_pred):\n",
    "    print(\"Model may be overfitting.\")\n",
    "else:\n",
    "    print(\"Model performance is consistent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large dataset, so we need to reduce the number of samples for SHAP\n",
    "background = shap.sample(X_train, 10000)\n",
    "\n",
    "# Initialize the SHAP Explainer\n",
    "explainer = shap.KernelExplainer(lr_model.predict_proba, background, link=\"logit\")\n",
    "\n",
    "# Compute SHAP values on a subset of the data to reduce computational load \n",
    "X_sample = shap.sample(X_train, 100)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "X_sample_dense = X_sample.toarray() if sp.issparse(X_sample) else X_sample\n",
    "\n",
    "# Plot the SHAP summary plot\n",
    "shap.summary_plot(shap_values[1], X_sample_dense, feature_names=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10) # Limit the depth to prevent overfitting and reduce overhead\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Training Classification Report:\\n\", classification_report(y_train, y_train_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Check for overfitting\n",
    "if accuracy_score(y_train, y_train_pred) > accuracy_score(y_test, y_test_pred):\n",
    "    print(\"Model may be overfitting.\")\n",
    "else:\n",
    "    print(\"Model performance is consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, max_depth=10, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Training Accuracy:\", accuracy_score(y_train, y_train_pred_xgb))\n",
    "print(\"XGBoost Test Accuracy:\", accuracy_score(y_test, y_test_pred_xgb))\n",
    "print(\"XGBoost Training Classification Report:\\n\", classification_report(y_train, y_train_pred_xgb))\n",
    "print(\"XGBoost Test Classification Report:\\n\", classification_report(y_test, y_test_pred_xgb))\n",
    "\n",
    "# Check for overfitting\n",
    "if accuracy_score(y_train, y_train_pred_xgb) > accuracy_score(y_test, y_test_pred_xgb):\n",
    "    print(\"XGBoost Model may be overfitting.\")\n",
    "else:\n",
    "    print(\"XGBoost Model performance is consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GloVe embeddings for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GloVe vectors\n",
    "\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            try:\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = vector\n",
    "            except ValueError:\n",
    "                print(f\"Error converting values for word: {word}\")\n",
    "                continue\n",
    "    return embeddings_index\n",
    "\n",
    "# Path to the GloVe file\n",
    "glove_path = 'glove.840B.300d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "print(f\"Loaded {len(glove_embeddings)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of the data due to size (1 million rows of text features)\n",
    "use_sample = True\n",
    "sample_fraction = 0.5\n",
    "\n",
    "# Decide whether to use a sample or the full dataset\n",
    "if use_sample:\n",
    "    df_used = df.sample(frac=sample_fraction, random_state=42)\n",
    "else:\n",
    "    df_used = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the predictor and target variables\n",
    "predictor = df_used['combined_text']\n",
    "target = df_used['Outlook Value'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictor, target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words=None)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Determine the maximum sequence length\n",
    "max_seq_length = max(len(x) for x in X_train_seq)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq_length)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix for the words we have in the dataset\n",
    "num_words = len(tokenizer.word_index) + 1 # Add 1 because of reserved 0 index for padding\n",
    "embedding_dim = 300  # Dimensionality of GloVe embeddings\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None: # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(f\"Shape of embedding matrix: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Embedding layer\n",
    "model.add(Embedding(input_dim=num_words, \n",
    "                    output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=max_seq_length, \n",
    "                    trainable=True))\n",
    "\n",
    "# Add the first LSTM layer\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# Add batch normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5)) # Can increase dropout to reduce overfitting\n",
    "\n",
    "# Add the second LSTM layer\n",
    "model.add(LSTM(128)) # Can decrease units to reduce overfitting\n",
    "\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks for early stopping and best model checkpointing\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_padded, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.25,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "# Print model training and validation progress\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, batch_size=128)\n",
    "\n",
    "# Print out the loss and accuracy on test data\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with LSTM + Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom layer for Attention\n",
    "class Attention(Layer):\n",
    "    def __init__(self, return_sequences=True): # return_sequences=True since using with LSTM\n",
    "        super(Attention, self).__init__()\n",
    "        self.return_sequences = return_sequences # Indicates whether to return the entire sequence\n",
    "\n",
    "    def build(self, input_shape): # Initialize the weights\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\")\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\")\n",
    "\n",
    "    def call(self, x): # Perform the operations to compute the attention weights\n",
    "        e = tf.nn.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1) # Apply softmax to the attention weights\n",
    "        output = x * a # Multiply the attention weights with the input data\n",
    "        \n",
    "        if self.return_sequences: # Return the entire sequence\n",
    "            return output\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "\n",
    "    def get_config(self): # Required for saving and loading the model\n",
    "        return {\"return_sequences\": self.return_sequences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the Embedding layer\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_seq_length,\n",
    "                    trainable=True))\n",
    "\n",
    "# Add the first LSTM layer\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# Add batch normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add the Attention layer\n",
    "model.add(Attention(return_sequences=True))\n",
    "\n",
    "# Add the second LSTM layer\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks for early stopping and best model checkpointing\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_model_with_attention.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train_padded, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.25,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "# Print model training and validation progress\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, batch_size=128)\n",
    "\n",
    "# Print out the loss and accuracy on test data\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode texts\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts.to_list(), padding=True, truncation=True, max_length=512, return_tensors=\"tf\")\n",
    "\n",
    "# Encode the predictor text\n",
    "encoded_inputs = encode_texts(df_used['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust labels from 1 and -1 to 1 and 0\n",
    "df_used['Outlook Value Adjusted'] = df_used['Outlook Value'].apply(lambda x: 1 if x == 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert adjusted labels to TensorFlow format\n",
    "labels = tf.convert_to_tensor(df_used['Outlook Value Adjusted'].tolist())\n",
    "\n",
    "# Create TensorFlow dataset\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "# Load tensors into the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((encoded_inputs['input_ids'], encoded_inputs['attention_mask'], labels))\n",
    "dataset = dataset.map(map_func).batch(32) # Can reduce batch size to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set\n",
    "train_size = int(0.75 * len(dataset))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks for early stopping and best model checkpointing\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_distilbert_model', monitor='val_loss', save_best_only=True, verbose=1, save_format='tf')\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Function to plot training and validation progress\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks for early stopping and best model checkpointing\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True),\n",
    "    ModelCheckpoint('best_distilbert_model', monitor='val_loss', save_best_only=True, verbose=1, save_format='tf')\n",
    "]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Function to plot training and validation progress\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
